{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bcfdc-404a-4f20-a220-f5acc6e612ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce4f7942-7092-431c-ba09-117f64efcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y) from raw data.\n",
    "\n",
    "#     Args:\n",
    "#         data (DataFrame): The raw dataset as a Pandas DataFrame.\n",
    "#         target_column (str): The name of the target column.\n",
    "\n",
    "#     Returns:\n",
    "#         X (DataFrame): Features.\n",
    "#         y (Series): Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c88f711-7183-4dac-b406-3eb644612663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, target_column=\"target\"):\n",
    "    data = pd.read_csv(file_path)  # Use file_path to read the dataset\n",
    "    X = data.drop(columns=[target_column])\n",
    "    y = data[target_column]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "840a6322-a023-47e1-bf7b-8ccb900b5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler.\n",
    "#     Args:\n",
    "#         X_train (DataFrame): Training features.\n",
    "#         X_test (DataFrame): Testing features.\n",
    "\n",
    "#     Returns:\n",
    "#         X_train_scaled (DataFrame): Scaled training features.\n",
    "#         X_test_scaled (DataFrame): Scaled testing features.\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e229133-cbd1-4328-b0b7-2a04738def2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafebb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function with metrics to analyze any trained model\n",
    "# Prints, confusion matrix, accuracy, precision, recall, f1 score, and other classification report metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Print accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49221266-3b71-47b8-935d-6654da68154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67726a2-3cf6-403d-a971-e1971ac759ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    predictions = lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Logistic Regression Accuracy:\", accuracy)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a3003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the best decision tree by using GridSearchCV to get best max_depth, min_samples_leaf, and max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_decision_tree(X_train, y_train):\n",
    "\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth': [5, 10, 15, 20],\n",
    "        'min_samples_leaf': [5, 10, 15, 20],\n",
    "        'max_features': [5, 10, 15]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_dt = grid_search.best_estimator_    \n",
    "    return best_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75f2d888-cfdc-4e34-921c-03e46f1004d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8eb304-0c64-401a-970e-f7d08d18f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X_train, y_train, X_test, y_test):\n",
    "    dt = get_best_decision_tree(X_train, y_train)\n",
    "    dt.fit(X_train, y_train)\n",
    "    predictions = dt.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    evaluate_predictions(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9038fa-588b-4e62-98d4-3a13ea8297f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the best KNN model by using GridSearchCV to get best K value and best weightage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_knn(X_train, y_train):\n",
    "    knn = KNeighborsClassifier(n_neighbors=min(5, len(X_train)))\n",
    "    param_grid = {\n",
    "        'n_neighbors': range(1, min(10, len(X_train)) + 1),\n",
    "        'weights': ['uniform', 'distance'],\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_knn = grid_search.best_estimator_\n",
    "    return best_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate a K-Nearest Neighbors (KNN) model. \n",
    "# Enhanced by using Grid Search CV to choose the best K value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad92c00-bd63-4152-8097-a1371fee7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(X_train, y_train, X_test, y_test):\n",
    "    best_knn = get_best_knn(X_train, y_train)\n",
    "    predictions = best_knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    evaluate_predictions(y_test, predictions)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54dbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_svm(X_train, y_train):\n",
    "    svm = SVC(random_state=42)\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'degree': [3, 4, 5]  # Only used for 'poly' kernel\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_svm = grid_search.best_estimator_   \n",
    "    return best_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    best_svm = get_best_svm(X_train, y_train)\n",
    "    predictions = best_svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    evaluate_predictions(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn_svm_ensemble(X_train, y_train, X_test, y_test):\n",
    "    best_knn = get_best_knn(X_train, y_train)\n",
    "    best_svm = get_best_svm(X_train, y_train)\n",
    "    base_learners = [\n",
    "        ('knn', best_knn),\n",
    "    ]\n",
    "    \n",
    "    ensemble_model = StackingClassifier(estimators=base_learners, final_estimator=best_svm)\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    predictions = ensemble_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    evaluate_predictions(y_test, predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_random_forest(X_train, y_train):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }    \n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_rf = grid_search.best_estimator_    \n",
    "    return best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(X_train, y_train, X_test, y_test):\n",
    "    rf = get_best_random_forest(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    evaluate_predictions(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c82251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_mlp(X_train, y_train):\n",
    "    mlp = MLPClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [200, 300]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "    print(\"Best Parameters for MLP:\", grid_search.best_params_)\n",
    "    return best_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dac00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(X_train, y_train, X_test, y_test):\n",
    "    best_mlp = get_best_mlp(X_train, y_train)\n",
    "    predictions = best_mlp.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    evaluate_predictions(y_test, predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1708126-026e-4ace-ab20-69e21049d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(file_path, target_column = \"target\"):\n",
    "    X, y = load_data(file_path, target_column) \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test)\n",
    "    \n",
    "    print(\"Training and Evaluating Models:\\n\")\n",
    "\n",
    "    print(\"\\nLogistic Regression:\")\n",
    "    train_logistic_regression(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nDecision Tree:\")\n",
    "    train_decision_tree(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nKNN:\")\n",
    "    train_knn(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nSVM:\")\n",
    "    train_svm(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nKNN + SVM Ensemble:\")\n",
    "    train_knn_svm_ensemble(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nRandom Forest:\")\n",
    "    train_random_forest(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "    \n",
    "    print(\"\\nMLP:\")\n",
    "    train_mlp(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline(\"expanded_dummy_stock_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fb5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f06fe7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPRegressor in module sklearn.neural_network._multilayer_perceptron:\n",
      "\n",
      "class MLPRegressor(sklearn.base.RegressorMixin, BaseMultilayerPerceptron)\n",
      " |  MLPRegressor(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |  \n",
      " |  Multi-layer Perceptron regressor.\n",
      " |  \n",
      " |  This model optimizes the squared error using LBFGS or stochastic gradient\n",
      " |  descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default='adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
      " |        Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      For a comparison between Adam optimizer and SGD, see\n",
      " |      :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`.\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Strength of the L2 regularization term. The L2 regularization term\n",
      " |      is divided by the sample size when added to the loss.\n",
      " |  \n",
      " |  batch_size : int, default='auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the regressor will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`.\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default='constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when solver='sgd'.\n",
      " |  \n",
      " |  learning_rate_init : float, default=0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : float, default=0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, default=200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Determines random number generation for weights and bias\n",
      " |      initialization, train-test split if early stopping is used, and batch\n",
      " |      sampling when solver='sgd' or 'adam'.\n",
      " |      Pass an int for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least ``tol`` for ``n_iter_no_change`` consecutive iterations,\n",
      " |      unless ``learning_rate`` is set to 'adaptive', convergence is\n",
      " |      considered to be reached and training stops.\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  momentum : float, default=0.9\n",
      " |      Momentum for gradient descent update. Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : bool, default=True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to True, it will automatically set\n",
      " |      aside ``validation_fraction`` of training data as validation and\n",
      " |      terminate training when validation score is not improving by at\n",
      " |      least ``tol`` for ``n_iter_no_change`` consecutive epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True.\n",
      " |  \n",
      " |  beta_1 : float, default=0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  beta_2 : float, default=0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'.\n",
      " |  \n",
      " |  epsilon : float, default=1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'.\n",
      " |  \n",
      " |  n_iter_no_change : int, default=10\n",
      " |      Maximum number of epochs to not meet ``tol`` improvement.\n",
      " |      Only effective when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  max_fun : int, default=15000\n",
      " |      Only used when solver='lbfgs'. Maximum number of function calls.\n",
      " |      The solver iterates until convergence (determined by ``tol``), number\n",
      " |      of iterations reaches max_iter, or this number of function calls.\n",
      " |      Note that number of function calls will be greater than or equal to\n",
      " |      the number of iterations for the MLPRegressor.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  best_loss_ : float\n",
      " |      The minimum loss reached by the solver throughout fitting.\n",
      " |      If `early_stopping=True`, this attribute is set to `None`. Refer to\n",
      " |      the `best_validation_score_` fitted attribute instead.\n",
      " |      Only accessible when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  loss_curve_ : list of shape (`n_iter_`,)\n",
      " |      Loss value evaluated at the end of each training step.\n",
      " |      The ith element in the list represents the loss at the ith iteration.\n",
      " |      Only accessible when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  validation_scores_ : list of shape (`n_iter_`,) or None\n",
      " |      The score at each iteration on a held-out validation set. The score\n",
      " |      reported is the R2 score. Only available if `early_stopping=True`,\n",
      " |      otherwise the attribute is set to `None`.\n",
      " |      Only accessible when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  best_validation_score_ : float or None\n",
      " |      The best validation score (i.e. R2 score) that triggered the\n",
      " |      early stopping. Only available if `early_stopping=True`, otherwise the\n",
      " |      attribute is set to `None`.\n",
      " |      Only accessible when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      The number of training samples seen by the solver during fitting.\n",
      " |      Mathematically equals `n_iters * X.shape[0]`, it means\n",
      " |      `time_step` and it is used by optimizer's learning rate scheduler.\n",
      " |  \n",
      " |  coefs_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list of shape (n_layers - 1,)\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The number of iterations the solver has run.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : str\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  BernoulliRBM : Bernoulli Restricted Boltzmann Machine (RBM).\n",
      " |  MLPClassifier : Multi-layer Perceptron classifier.\n",
      " |  sklearn.linear_model.SGDRegressor : Linear model fitted by minimizing\n",
      " |      a regularized empirical loss with SGD.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPRegressor trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense and sparse numpy\n",
      " |  arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E. \"Connectionist learning procedures.\"\n",
      " |  Artificial intelligence 40.1 (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio.\n",
      " |  \"Understanding the difficulty of training deep feedforward neural networks.\"\n",
      " |  International Conference on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  :arxiv:`He, Kaiming, et al (2015). \"Delving deep into rectifiers:\n",
      " |  Surpassing human-level performance on imagenet classification.\" <1502.01852>`\n",
      " |  \n",
      " |  :arxiv:`Kingma, Diederik, and Jimmy Ba (2014)\n",
      " |  \"Adam: A method for stochastic optimization.\" <1412.6980>`\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.neural_network import MLPRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> from sklearn.model_selection import train_test_split\n",
      " |  >>> X, y = make_regression(n_samples=200, random_state=1)\n",
      " |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      " |  ...                                                     random_state=1)\n",
      " |  >>> regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
      " |  >>> regr.predict(X_test[:2])\n",
      " |  array([-0.9..., -7.1...])\n",
      " |  >>> regr.score(X_test, y_test)\n",
      " |  0.4...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseMultilayerPerceptron\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  partial_fit(self, X, y)\n",
      " |      Update the model with a single iteration over the given data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Trained MLP model.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.neural_network._multilayer_perceptron.MLPRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neural_network._multilayer_perceptron.MLPRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix of shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPRegressor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
