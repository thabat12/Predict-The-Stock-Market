{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.io as pio\n",
    "from plotly.offline import plot\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Some Useful Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# financial indicator functions\n",
    "# note that these are indented to work with ONE key value\n",
    "# ex. ONLY DATES means that we must group by stock ticker\n",
    "# note: TICKERS will not make sense for these functions as keys\n",
    "\n",
    "def check_series_numeric(column) -> bool:\n",
    "    not_numeric = column.apply(pd.to_numeric, errors = 'coerce').isna().all()\n",
    "    # if the data is not numeric, the test failed \n",
    "    return not not_numeric\n",
    "\n",
    "def calculate_sma(column, period = 10):\n",
    "    if not check_series_numeric(column):\n",
    "        return column\n",
    "    sma = column.rolling(period).mean()\n",
    "    return pd.Series(sma, index= column.index, name = 'SMA')\n",
    "\n",
    "def calculate_ema(column, period = 10):\n",
    "    if not check_series_numeric(column):\n",
    "        return column\n",
    "    ema = column.ewm(span = period, min_periods = period - 1).mean()\n",
    "    return pd.Series(ema, index = column.index, name = 'EMA')\n",
    "\n",
    "def calculate_rsi_helper(column, period = 14, use_exponential = False):\n",
    "    if not check_series_numeric(column):\n",
    "        return column \n",
    "\n",
    "    column = column.astype(float)\n",
    "    # find differences in prices\n",
    "    differences = column.diff() \n",
    "\n",
    "    # clip, but center off of zero value\n",
    "    gain = differences.clip(lower = 0.01)\n",
    "    loss = differences.clip(upper = -0.01)\n",
    "\n",
    "    avg_gain = gain.mean()\n",
    "    avg_loss = loss.mean()\n",
    "\n",
    "    if use_exponential:\n",
    "        # get the exponential weighted mean of the very last element in this current rolling window\n",
    "        avg_gain = gain.ewm(span = period, min_periods = period - 1).mean().iloc[-1]\n",
    "        avg_loss = loss.ewm(span = period, min_periods = period - 1).mean().iloc[-1]\n",
    "\n",
    "    RS = avg_gain / avg_loss \n",
    "    RSI = 100 - 100 / (1 - RS)\n",
    "\n",
    "    return RSI\n",
    "\n",
    "'''\n",
    "    let RS = average gain / average loss   \n",
    "    RSI = 100 - 100 / (1 - RS)\n",
    "\n",
    "    some things to look out for: \n",
    "        - using exponential weighted means on the average\n",
    "            gains and losses made the RSI value much more\n",
    "            sensitive and fluctuating more based on price\n",
    "            changes\n",
    "        - using simple averages made the RSI value more \n",
    "            smoothed out\n",
    "\n",
    "    keyword argument options:\n",
    "        - period: type int\n",
    "        - use_exponential: type bool\n",
    "\n",
    "'''\n",
    "def calculate_rsi(column, period, use_exponential):\n",
    "    rsi = column.rolling(14).apply(lambda x: calculate_rsi_helper(x, period=period, use_exponential=use_exponential)).astype(float)\n",
    "    return pd.Series(rsi, index = column.index, name = 'RSI')\n",
    "\n",
    "'''\n",
    "    MACD (moving average convergence/ divergence) shows the relationship\n",
    "    between two exponential moving averages and comparing this to the \n",
    "    9-day EMA line \n",
    "\n",
    "    MACD = 12-period EMA - 26-period EMA \n",
    "    Signal = 9-period EMA - 26-period EMA\n",
    "'''\n",
    "def calculate_macd(column, long_period = 26, short_period = 12, signal_period = 9) -> tuple[object, object]:\n",
    "    if not check_series_numeric(column):\n",
    "        return column \n",
    "    \n",
    "    # Calculate the short and long EMAs\n",
    "    ewm_short = column.ewm(span=short_period, min_periods=short_period-1).mean()\n",
    "    ewm_long = column.ewm(span=long_period, min_periods=long_period-1).mean()\n",
    "\n",
    "    # Calculate MACD line\n",
    "    macd = (ewm_short - ewm_long).astype(float)\n",
    "\n",
    "    # Calculate the Signal line (EMA of MACD)\n",
    "    ewm_signal = macd.ewm(span=signal_period, min_periods=signal_period-1).mean()\n",
    "    signal = ewm_signal.astype(float)\n",
    "\n",
    "    # Return both MACD and Signal as pandas Series\n",
    "    macd = pd.Series(macd, index=column.index, name='MACD')\n",
    "    signal = pd.Series(signal, index=column.index, name='MACD-SIGNAL')\n",
    "\n",
    "    return macd, signal\n",
    "\n",
    "\"\"\"\n",
    "    Calculate the average stock price between 30 and 45 days after the present day.\n",
    "    Expects that data is given in sorted order by date.\n",
    "\n",
    "    - data: pd.DataFrame with stock data indexed by date.\n",
    "    - column_name: str, the column name containing the stock prices.\n",
    "    This returns the average stock price between 30 and 45 days after the present day.\n",
    "\"\"\"\n",
    "def calculate_future_average_stock_price(column, start_period=10, end_period=30) -> pd.Series:\n",
    "    \"\"\"\n",
    "        Calculate the average stock price between +start_period and +end_period days from the \n",
    "        present day.\n",
    "    \n",
    "        :param column: pd.Series, the stock prices.\n",
    "        :param start_period: int, the start period.\n",
    "        :param end_period: int, the end period.\n",
    "\n",
    "        For \n",
    "\n",
    "    \"\"\"\n",
    "    MINTHRESH_PROP = 0.5\n",
    "\n",
    "    future_average_prices = []\n",
    "\n",
    "    for i in range(len(column)):\n",
    "        cur_date = column.index[i]\n",
    "\n",
    "        future_dates = [cur_date + datetime.timedelta(days=j) \\\n",
    "                        for j in range(start_period, end_period + 1)]\n",
    "        valid_future_dates = [date for date in future_dates if date in column.index]\n",
    "\n",
    "        future_prices = column.loc[valid_future_dates]\n",
    "        # weekends are not included in the future prices, but that's okay as long as we ensure that\n",
    "        # all weekend days missing are accounted for\n",
    "\n",
    "        # 0 = Monday, ... 6 = Saturday, 7 = Sunday\n",
    "        day_of_week_future_dates = [int(date.weekday()) for date in future_dates]\n",
    "        num_weekdays = sum([int(day_num <= 4) for day_num in day_of_week_future_dates])\n",
    "        # print(\"expected weekdays:\", num_weekdays, \"weekday in loc\", len(future_prices))\n",
    "\n",
    "        num_future_prices = len(future_prices)\n",
    "\n",
    "        # add price to the future price average list if there are sufficient or exactly the same\n",
    "        # number of samples as expected for stock prices in the future\n",
    "        if num_future_prices == num_weekdays or num_future_prices > \\\n",
    "            int((end_period - start_period + 1) * MINTHRESH_PROP):\n",
    "            future_average_prices.append(future_prices.mean())\n",
    "        else:\n",
    "            future_average_prices.append(None)\n",
    "            \n",
    "    future_average_prices = pd.Series(future_average_prices, index=column.index, \n",
    "                                      name=f\"FutureAverage{start_period}-{end_period}\")\n",
    "\n",
    "    return future_average_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Utility Functions Used Throughout the Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Data cleaning and feature engineering the tickers dataframe:\n",
    "        - I only ned to know Adj Close and Volume numbers for relevant metrics per each stock\n",
    "        - The stocks of interest are:\n",
    "            - Stock that I am predicting for trading\n",
    "            - Probably some other related stocks (2-6 of them)\n",
    "            - Global market conditions indicators\n",
    "\"\"\"\n",
    "def extract_tickers_ticker(\n",
    "        tickers_df: pd.DataFrame, tickers: list[str], \n",
    "        start: datetime.datetime, end: datetime.datetime) -> pd.DataFrame:\n",
    "\n",
    "    res_df = tickers_df[tickers]\n",
    "    cols_to_keep = set([\"Adj Close\", \"Volume\"])\n",
    "    remapped_columns = [f\"{outer}:{inner}\" for outer, inner in res_df.columns]\n",
    "    apple_cols_to_keep = [\"Ticker:Price\"] + \\\n",
    "        [f\"{outer}:{inner}\" for outer, inner in res_df.columns if inner in cols_to_keep]\n",
    "    res_df.columns = remapped_columns\n",
    "    res_df = res_df[apple_cols_to_keep]\n",
    "    res_df = res_df.iloc[1:]\n",
    "    res_df[\"Ticker:Price\"] = pd.to_datetime(res_df[\"Ticker:Price\"])\n",
    "    res_df = res_df[(res_df[\"Ticker:Price\"] >= start) & (res_df[\"Ticker:Price\"] <= end)]\n",
    "    res_df = res_df.set_index(\"Ticker:Price\").rename_axis(\"Date\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def parse_technical_indicators(\n",
    "        tickers_df: pd.DataFrame, tickers: list[str], ticker: str,\n",
    "        start: datetime.datetime, end: datetime.datetime,\n",
    "        short_period: int = 15, long_period: int = 30) -> pd.DataFrame:\n",
    "    \n",
    "    res_df = extract_tickers_ticker(tickers_df, tickers, start, end)\n",
    "\n",
    "    # get the Y value: future average stock price\n",
    "    res_df[\"future_avg_price\"] = calculate_future_average_stock_price(res_df[f\"{ticker}:Adj Close\"])\n",
    "    cols = res_df.columns\n",
    "    for colname in cols:\n",
    "        if \"Volume\" in colname:\n",
    "            # get the ewn of volumes\n",
    "            res_df[colname + \"_ewm_short\"] = res_df[colname].ewm(span=short_period, adjust=False).mean()\n",
    "            res_df[colname + \"_ewm_long\"] = res_df[colname].ewm(span=long_period, adjust=False).mean()\n",
    "        elif \"Adj Close\" in colname:\n",
    "            # get RSI & MACD signal difference otherwise\n",
    "            res_df[colname + \"_rsi\"] = calculate_rsi(res_df[colname], period=14, use_exponential=True)\n",
    "            macd, signal = calculate_macd(res_df[colname])\n",
    "            res_df[colname + \"_macd\"] = macd\n",
    "            res_df[colname + \"_signal\"] = signal\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def extract_congressional_trades_ticker(congressional_df: pd.DataFrame, tickers: list[str]) -> pd.DataFrame:\n",
    "\n",
    "    res_df = filter_congressional_trades_ticker(congressional_df, tickers)\n",
    "    res_df[\"disclosure_date\"] = pd.to_datetime(res_df[\"disclosure_date\"])\n",
    "\n",
    "    res_df[\"type\"] = res_df[\"type\"].str.lower()\n",
    "    res_df = res_df[~res_df[\"type\"].str.contains(\"exchange\")]\n",
    "\n",
    "    res_df[\"type\"] = res_df[\"type\"].str.contains(\"purchase\", na=False) # True if purchase\n",
    "    smmap = {True: \"purchase\", False: \"sale\"}\n",
    "    res_df[\"type\"] = res_df[\"type\"].map(smmap)\n",
    "\n",
    "    res_df.set_index(\"disclosure_date\", inplace=True)\n",
    "\n",
    "    res_df = res_df[[\"ticker\", \"type\", \"amount\"]]\n",
    "\n",
    "    res_df[\"count\"] = 1\n",
    "    res_df = res_df.pivot_table(\n",
    "        columns=\"type\", \n",
    "        values=\"count\", \n",
    "        aggfunc=\"sum\", \n",
    "        index=[\"disclosure_date\", \"ticker\"],\n",
    "        ).fillna(0)\n",
    "\n",
    "    res_df = res_df.rename_axis(columns=None).reset_index()\n",
    "    res_df = res_df.set_index(\"disclosure_date\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def filter_congressional_trades_ticker(congressional_df: pd.DataFrame, tickers: list[str]) -> pd.DataFrame:\n",
    "    filtered_df = congressional_df[congressional_df[\"ticker\"].notna()]\n",
    "    filtered_df = filtered_df[filtered_df[\"ticker\"].str.contains('|'.join(tickers))]\n",
    "    filtered_df.drop([\"Unnamed: 0\"], inplace=True, axis=1)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Functions for Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the technical indicators of volume, RSI, and MACD_diff\n",
    "def plot_technical_indicators(\n",
    "        stock_data_df: pd.DataFrame, start: datetime.datetime, end: datetime.datetime,\n",
    "        ticker_name: str) -> None:\n",
    "    ticker_name = ticker_name.upper()\n",
    "    plot_df = stock_data_df[(stock_data_df.index >= start) & (stock_data_df.index <= end)]\n",
    "\n",
    "    trace_price = go.Scatter(x=plot_df.index, y=plot_df[f\"{ticker_name}:Adj Close\"], mode='lines', name='Price')\n",
    "    trace_rsi = go.Scatter(x=plot_df.index, y=plot_df[f\"{ticker_name}:Adj Close_rsi\"], mode='lines', name='RSI')\n",
    "    trace_overbought = go.Scatter(x=plot_df.index, y=[70] * len(plot_df), mode='lines', name='Overbought', line=dict(color='red', dash='dash'))\n",
    "    trace_oversold = go.Scatter(x=plot_df.index, y=[30] * len(plot_df), mode='lines', name='Oversold', line=dict(color='green', dash='dash'))\n",
    "    trace_macd = go.Scatter(x=plot_df.index, y=plot_df[f\"{ticker_name}:Adj Close_macd\"], mode='lines', name='MACD')\n",
    "    trace_signal = go.Scatter(x=plot_df.index, y=plot_df[f\"{ticker_name}:Adj Close_signal\"], mode='lines', name='Signal')\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "                        subplot_titles=[\"Stock Price\", \"RSI\", \"MACD\"])\n",
    "\n",
    "    fig.add_trace(trace_price, row=1, col=1)\n",
    "    fig.add_trace(trace_rsi, row=2, col=1)\n",
    "    fig.add_trace(trace_overbought, row=2, col=1)\n",
    "    fig.add_trace(trace_oversold, row=2, col=1)\n",
    "    fig.add_trace(trace_macd, row=3, col=1)\n",
    "    fig.add_trace(trace_signal, row=3, col=1)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"${ticker_name} Technical Indicators\",\n",
    "        title_font=dict(color=\"white\", size=20),\n",
    "        \n",
    "        # Dark background settings\n",
    "        plot_bgcolor=\"#1c1c1c\",  # Dark background for the plot area\n",
    "        paper_bgcolor=\"#121212\",  # Dark background for the surrounding area\n",
    "        \n",
    "        # Axes and gridline settings\n",
    "        xaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor=\"#444444\",  # Gridline color\n",
    "            tickangle=45,\n",
    "            tickmode=\"array\",\n",
    "            tickfont=dict(color=\"white\")\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridcolor=\"#444444\",  # Gridline color\n",
    "            tickfont=dict(color=\"white\"),\n",
    "        ),\n",
    "        \n",
    "        # Additional customizations for ticks, labels, etc.\n",
    "        font=dict(color=\"white\"),  # Set general text color to white\n",
    "        height=800, width=800\n",
    "    )\n",
    "    \n",
    "    # pip install -U kaledio\n",
    "    pio.renderers.default = \"png\"\n",
    "    fig.show()\n",
    "\n",
    "# for macd, if the macd line crosses the signal line, that is a bullish crossover\n",
    "# if the macd line crosses below the signal line, that is a bearish crossover\n",
    "\n",
    "def plot_congressional_trades_bar(congressional_df: pd.DataFrame, tickers: list[str]) -> None:\n",
    "    \"\"\" \n",
    "        Given the raw congressional trading dataframe, plot the frequency of trades\n",
    "        for either a sale or a purchase of the stock. This function will naively assume\n",
    "        that any type of purchase is a \"buy\", whereas any type of sell is a \"sell\",\n",
    "        despite the fact that there are more subleties to this where one can either\n",
    "        partially buy or sell a stock at varying exchange amounts.\n",
    "    \"\"\"\n",
    "\n",
    "    ## some data exploration on the apple congressional data\n",
    "    filtered_df = filter_congressional_trades_ticker(congressional_df, tickers)\n",
    "\n",
    "    summarized_cong_df = filtered_df.groupby([\"amount\", \"type\"]).size().reset_index()\n",
    "    summarized_cong_df.columns = [\"amount\", \"type\", \"count\"]\n",
    "    unique_amounts = summarized_cong_df[\"amount\"].unique()\n",
    "    unique_amounts_sorted = sorted(unique_amounts, key=lambda x: \\\n",
    "        int([str_x for str_x in x.replace('$', '').replace(',', '').split(' ') if str_x.isnumeric()][-1]))\n",
    "\n",
    "    summarized_cong_df[\"type\"] = summarized_cong_df[\"type\"].str.lower()\n",
    "    # i am not considering \"exchange\" as a valid trade option\n",
    "    summarized_cong_df = summarized_cong_df[~summarized_cong_df[\"type\"].str.contains(\"exchange\")]\n",
    "    summarized_cong_df[\"type\"] = summarized_cong_df[\"type\"].str.contains(\"purchase\", na=False) # True if purchase\n",
    "    smmap = {True: \"purchase\", False: \"sale\"}\n",
    "    summarized_cong_df[\"type\"] = summarized_cong_df[\"type\"].map(smmap)\n",
    "\n",
    "    purchase_types = summarized_cong_df.groupby([\"amount\", \"type\"]).sum().reset_index()\n",
    "\n",
    "    def _extract_purchase_sell(X: pd.DataFrame):\n",
    "        purchase_amt = X[X[\"type\"].str.contains(\"purchase\")][\"count\"].sum()\n",
    "        sell_amt = X[X[\"type\"].str.contains(\"sale\")][\"count\"].sum()\n",
    "        return pd.Series([purchase_amt, sell_amt], index=[\"purchase\", \"sale\"])\n",
    "\n",
    "    DARKGRAY = \"#1c1c1c\"\n",
    "    GREEN = \"#46cb46\"\n",
    "    RED = \"#e73f3f\"\n",
    "\n",
    "    purchase_types = purchase_types.groupby(\"amount\").apply(_extract_purchase_sell)\n",
    "    purchase_types = purchase_types.reindex(unique_amounts_sorted)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bottom = np.zeros(len(unique_amounts))\n",
    "\n",
    "    for amt_type in [\"sale\", \"purchase\"]:\n",
    "        ax.bar(purchase_types.index, purchase_types[amt_type], label=amt_type, color = GREEN if amt_type == \"purchase\" else RED, bottom=bottom)\n",
    "        bottom += purchase_types[amt_type].to_numpy()\n",
    "\n",
    "    # Create the plot\n",
    "    fig.patch.set_facecolor(DARKGRAY)\n",
    "    ax.set_facecolor(DARKGRAY)\n",
    "\n",
    "    # Add gridlines and customize ticks\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6, color=\"gray\")\n",
    "    ax.tick_params(axis=\"x\", rotation=80, colors=\"white\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"white\")\n",
    "\n",
    "\n",
    "    # Set title with a line break\n",
    "    title_text = f\"Frequency of Congressional\\nTrade Amount Categories For: \\n${', '.join(tickers)}\"\n",
    "    ax.set_title(title_text, fontsize=16, weight='bold', color=\"white\", pad=20)\n",
    "\n",
    "    # Add labels for x and y axes\n",
    "    ax.set_xlabel(\"Trade Amount Categories\", fontsize=12, color=\"white\")\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=12, color=\"white\", labelpad=10)\n",
    "\n",
    "    # Set axis lines (spines) to white\n",
    "    ax.spines['bottom'].set_color('white')  # X-axis line\n",
    "    ax.spines['left'].set_color('white')    # Y-axis line\n",
    "    ax.spines['top'].set_color('white')     # Hide top line\n",
    "    ax.spines['right'].set_color('white')   # Hide right line\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend(title=\"Trade Type\")\n",
    "\n",
    "    # Adjust the layout to prevent label clipping\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_congressional_trades_timeseries(\n",
    "        training_ticker_congressional_df: pd.DataFrame, start: datetime.datetime, end: datetime.datetime,\n",
    "        ticker: str, title: str = 'Future Avg Price and Transaction Volume Over Time', barwidth: int = 15) -> None:\n",
    "\n",
    "    sub_df = training_ticker_congressional_df[(training_ticker_congressional_df.index >= start) & \\\n",
    "                                              (training_ticker_congressional_df.index <= end)]\n",
    "\n",
    "    # Visualize the merged dataframe with congressional and time series insights\n",
    "    fig, ax1 = plt.subplots(figsize=(14,7))\n",
    "\n",
    "    ax1.plot(sub_df.index, sub_df['future_avg_price'], label='Future Avg Price', color='blue', linewidth=2)\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Future Avg Price', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_title(title)\n",
    "\n",
    "    # Create a secondary y-axis for transaction counts\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Adjust bar width and offsets\n",
    "    width = np.timedelta64(10, 'D')  # Offset width for separation (10 days)\n",
    "\n",
    "    # Plot Purchases\n",
    "    ax2.bar(\n",
    "        sub_df.index,  # Shift purchases bars to the left\n",
    "        sub_df[f\"purchase_{ticker}\"],\n",
    "        color='green',\n",
    "        alpha=0.6,\n",
    "        width=barwidth,  # Reduce bar width\n",
    "        label='Purchases'\n",
    "    )\n",
    "\n",
    "    # Plot Sells\n",
    "    ax2.bar(\n",
    "        sub_df.index,  # Shift sells bars to the right\n",
    "        sub_df[f\"sale_{ticker}\"],\n",
    "        color='red',\n",
    "        alpha=0.6,\n",
    "        width=barwidth,  # Reduce bar width\n",
    "        label='Sells'\n",
    "    )\n",
    "\n",
    "    # # Set y-axis label for transactions\n",
    "    ax2.set_ylabel('Transaction Volume', color='gray')\n",
    "    ax2.tick_params(axis='y', labelcolor='gray')\n",
    "\n",
    "    # # Add legends for both plots\n",
    "    fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare AAPL Tickers & Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL:Adj Close</th>\n",
       "      <th>AAPL:Volume</th>\n",
       "      <th>MSFT:Adj Close</th>\n",
       "      <th>MSFT:Volume</th>\n",
       "      <th>AMZN:Adj Close</th>\n",
       "      <th>AMZN:Volume</th>\n",
       "      <th>GOOGL:Adj Close</th>\n",
       "      <th>GOOGL:Volume</th>\n",
       "      <th>META:Adj Close</th>\n",
       "      <th>META:Volume</th>\n",
       "      <th>...</th>\n",
       "      <th>DX-Y.NYB:Adj Close_rsi</th>\n",
       "      <th>DX-Y.NYB:Adj Close_macd</th>\n",
       "      <th>DX-Y.NYB:Adj Close_signal</th>\n",
       "      <th>DX-Y.NYB:Volume_ewm_short</th>\n",
       "      <th>DX-Y.NYB:Volume_ewm_long</th>\n",
       "      <th>HG=F:Adj Close_rsi</th>\n",
       "      <th>HG=F:Adj Close_macd</th>\n",
       "      <th>HG=F:Adj Close_signal</th>\n",
       "      <th>HG=F:Volume_ewm_short</th>\n",
       "      <th>HG=F:Volume_ewm_long</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-05-28</th>\n",
       "      <td>19.668386</td>\n",
       "      <td>315481600.0</td>\n",
       "      <td>33.929050</td>\n",
       "      <td>25711500.0</td>\n",
       "      <td>15.508000</td>\n",
       "      <td>54520000.0</td>\n",
       "      <td>28.452047</td>\n",
       "      <td>31632000.0</td>\n",
       "      <td>63.319351</td>\n",
       "      <td>47795000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.268010</td>\n",
       "      <td>0.163706</td>\n",
       "      <td>0.082837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.149754</td>\n",
       "      <td>0.026384</td>\n",
       "      <td>0.021185</td>\n",
       "      <td>1065.134806</td>\n",
       "      <td>1181.273277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-29</th>\n",
       "      <td>20.026762</td>\n",
       "      <td>376474000.0</td>\n",
       "      <td>34.208897</td>\n",
       "      <td>19888200.0</td>\n",
       "      <td>15.689000</td>\n",
       "      <td>47310000.0</td>\n",
       "      <td>28.457535</td>\n",
       "      <td>28222000.0</td>\n",
       "      <td>63.638393</td>\n",
       "      <td>42700000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.586279</td>\n",
       "      <td>0.178079</td>\n",
       "      <td>0.101885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.309509</td>\n",
       "      <td>0.024091</td>\n",
       "      <td>0.021767</td>\n",
       "      <td>1039.513206</td>\n",
       "      <td>1164.444645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-30</th>\n",
       "      <td>19.951740</td>\n",
       "      <td>564020800.0</td>\n",
       "      <td>34.717697</td>\n",
       "      <td>34567600.0</td>\n",
       "      <td>15.627500</td>\n",
       "      <td>85204000.0</td>\n",
       "      <td>28.511900</td>\n",
       "      <td>37434000.0</td>\n",
       "      <td>63.109982</td>\n",
       "      <td>45253500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.473042</td>\n",
       "      <td>0.177741</td>\n",
       "      <td>0.117056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.161556</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>992.899451</td>\n",
       "      <td>1136.659574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-02</th>\n",
       "      <td>19.814634</td>\n",
       "      <td>369350800.0</td>\n",
       "      <td>34.590504</td>\n",
       "      <td>18504300.0</td>\n",
       "      <td>15.442000</td>\n",
       "      <td>44068000.0</td>\n",
       "      <td>28.147303</td>\n",
       "      <td>33210000.0</td>\n",
       "      <td>62.890648</td>\n",
       "      <td>35996000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>70.302560</td>\n",
       "      <td>0.196982</td>\n",
       "      <td>0.133041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.148407</td>\n",
       "      <td>0.021102</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>949.422067</td>\n",
       "      <td>1109.851064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-03</th>\n",
       "      <td>20.094843</td>\n",
       "      <td>292709200.0</td>\n",
       "      <td>34.166500</td>\n",
       "      <td>18068900.0</td>\n",
       "      <td>15.359500</td>\n",
       "      <td>47584000.0</td>\n",
       "      <td>27.657017</td>\n",
       "      <td>40626000.0</td>\n",
       "      <td>62.681274</td>\n",
       "      <td>32217000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64.079381</td>\n",
       "      <td>0.202635</td>\n",
       "      <td>0.146960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.729535</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>0.020874</td>\n",
       "      <td>903.975482</td>\n",
       "      <td>1081.495291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-04</th>\n",
       "      <td>20.324303</td>\n",
       "      <td>335482000.0</td>\n",
       "      <td>34.191936</td>\n",
       "      <td>23209000.0</td>\n",
       "      <td>15.339000</td>\n",
       "      <td>42142000.0</td>\n",
       "      <td>27.619610</td>\n",
       "      <td>34582000.0</td>\n",
       "      <td>63.149864</td>\n",
       "      <td>36514000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.109968</td>\n",
       "      <td>0.214324</td>\n",
       "      <td>0.160433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.041064</td>\n",
       "      <td>0.012218</td>\n",
       "      <td>0.019143</td>\n",
       "      <td>865.718999</td>\n",
       "      <td>1056.233151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-05</th>\n",
       "      <td>20.404051</td>\n",
       "      <td>303805600.0</td>\n",
       "      <td>34.946674</td>\n",
       "      <td>31865200.0</td>\n",
       "      <td>16.178499</td>\n",
       "      <td>155934000.0</td>\n",
       "      <td>28.176729</td>\n",
       "      <td>35868000.0</td>\n",
       "      <td>63.000313</td>\n",
       "      <td>47352000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.485060</td>\n",
       "      <td>0.197115</td>\n",
       "      <td>0.167769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.751111</td>\n",
       "      <td>0.006907</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>826.382290</td>\n",
       "      <td>1029.995998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-06</th>\n",
       "      <td>20.347939</td>\n",
       "      <td>349938400.0</td>\n",
       "      <td>35.175640</td>\n",
       "      <td>24060500.0</td>\n",
       "      <td>16.483500</td>\n",
       "      <td>104880000.0</td>\n",
       "      <td>28.231594</td>\n",
       "      <td>34804000.0</td>\n",
       "      <td>62.312386</td>\n",
       "      <td>42442000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51.421188</td>\n",
       "      <td>0.184575</td>\n",
       "      <td>0.171130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.478462</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>800.293110</td>\n",
       "      <td>1010.061703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-09</th>\n",
       "      <td>20.673534</td>\n",
       "      <td>301660000.0</td>\n",
       "      <td>34.997555</td>\n",
       "      <td>15019200.0</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>64244000.0</td>\n",
       "      <td>28.466015</td>\n",
       "      <td>30594000.0</td>\n",
       "      <td>62.691242</td>\n",
       "      <td>37617000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.645812</td>\n",
       "      <td>0.191788</td>\n",
       "      <td>0.175262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.801890</td>\n",
       "      <td>-0.006654</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>765.370973</td>\n",
       "      <td>985.436729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-06-10</th>\n",
       "      <td>20.794886</td>\n",
       "      <td>251108000.0</td>\n",
       "      <td>34.861870</td>\n",
       "      <td>15117700.0</td>\n",
       "      <td>16.620501</td>\n",
       "      <td>73268000.0</td>\n",
       "      <td>28.344814</td>\n",
       "      <td>29676000.0</td>\n",
       "      <td>65.572571</td>\n",
       "      <td>69338000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66.828676</td>\n",
       "      <td>0.208813</td>\n",
       "      <td>0.181972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.189286</td>\n",
       "      <td>-0.010748</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>731.540588</td>\n",
       "      <td>961.028968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            AAPL:Adj Close  AAPL:Volume  MSFT:Adj Close  MSFT:Volume  \\\n",
       "Date                                                                   \n",
       "2014-05-28       19.668386  315481600.0       33.929050   25711500.0   \n",
       "2014-05-29       20.026762  376474000.0       34.208897   19888200.0   \n",
       "2014-05-30       19.951740  564020800.0       34.717697   34567600.0   \n",
       "2014-06-02       19.814634  369350800.0       34.590504   18504300.0   \n",
       "2014-06-03       20.094843  292709200.0       34.166500   18068900.0   \n",
       "2014-06-04       20.324303  335482000.0       34.191936   23209000.0   \n",
       "2014-06-05       20.404051  303805600.0       34.946674   31865200.0   \n",
       "2014-06-06       20.347939  349938400.0       35.175640   24060500.0   \n",
       "2014-06-09       20.673534  301660000.0       34.997555   15019200.0   \n",
       "2014-06-10       20.794886  251108000.0       34.861870   15117700.0   \n",
       "\n",
       "            AMZN:Adj Close  AMZN:Volume  GOOGL:Adj Close  GOOGL:Volume  \\\n",
       "Date                                                                     \n",
       "2014-05-28       15.508000   54520000.0        28.452047    31632000.0   \n",
       "2014-05-29       15.689000   47310000.0        28.457535    28222000.0   \n",
       "2014-05-30       15.627500   85204000.0        28.511900    37434000.0   \n",
       "2014-06-02       15.442000   44068000.0        28.147303    33210000.0   \n",
       "2014-06-03       15.359500   47584000.0        27.657017    40626000.0   \n",
       "2014-06-04       15.339000   42142000.0        27.619610    34582000.0   \n",
       "2014-06-05       16.178499  155934000.0        28.176729    35868000.0   \n",
       "2014-06-06       16.483500  104880000.0        28.231594    34804000.0   \n",
       "2014-06-09       16.375000   64244000.0        28.466015    30594000.0   \n",
       "2014-06-10       16.620501   73268000.0        28.344814    29676000.0   \n",
       "\n",
       "            META:Adj Close  META:Volume  ...  DX-Y.NYB:Adj Close_rsi  \\\n",
       "Date                                     ...                           \n",
       "2014-05-28       63.319351   47795000.0  ...               82.268010   \n",
       "2014-05-29       63.638393   42700000.0  ...               71.586279   \n",
       "2014-05-30       63.109982   45253500.0  ...               60.473042   \n",
       "2014-06-02       62.890648   35996000.0  ...               70.302560   \n",
       "2014-06-03       62.681274   32217000.0  ...               64.079381   \n",
       "2014-06-04       63.149864   36514000.0  ...               69.109968   \n",
       "2014-06-05       63.000313   47352000.0  ...               49.485060   \n",
       "2014-06-06       62.312386   42442000.0  ...               51.421188   \n",
       "2014-06-09       62.691242   37617000.0  ...               61.645812   \n",
       "2014-06-10       65.572571   69338000.0  ...               66.828676   \n",
       "\n",
       "            DX-Y.NYB:Adj Close_macd  DX-Y.NYB:Adj Close_signal  \\\n",
       "Date                                                             \n",
       "2014-05-28                 0.163706                   0.082837   \n",
       "2014-05-29                 0.178079                   0.101885   \n",
       "2014-05-30                 0.177741                   0.117056   \n",
       "2014-06-02                 0.196982                   0.133041   \n",
       "2014-06-03                 0.202635                   0.146960   \n",
       "2014-06-04                 0.214324                   0.160433   \n",
       "2014-06-05                 0.197115                   0.167769   \n",
       "2014-06-06                 0.184575                   0.171130   \n",
       "2014-06-09                 0.191788                   0.175262   \n",
       "2014-06-10                 0.208813                   0.181972   \n",
       "\n",
       "            DX-Y.NYB:Volume_ewm_short  DX-Y.NYB:Volume_ewm_long  \\\n",
       "Date                                                              \n",
       "2014-05-28                        0.0                       0.0   \n",
       "2014-05-29                        0.0                       0.0   \n",
       "2014-05-30                        0.0                       0.0   \n",
       "2014-06-02                        0.0                       0.0   \n",
       "2014-06-03                        0.0                       0.0   \n",
       "2014-06-04                        0.0                       0.0   \n",
       "2014-06-05                        0.0                       0.0   \n",
       "2014-06-06                        0.0                       0.0   \n",
       "2014-06-09                        0.0                       0.0   \n",
       "2014-06-10                        0.0                       0.0   \n",
       "\n",
       "            HG=F:Adj Close_rsi  HG=F:Adj Close_macd  HG=F:Adj Close_signal  \\\n",
       "Date                                                                         \n",
       "2014-05-28           57.149754             0.026384               0.021185   \n",
       "2014-05-29           49.309509             0.024091               0.021767   \n",
       "2014-05-30           45.161556             0.020624               0.021538   \n",
       "2014-06-02           54.148407             0.021102               0.021451   \n",
       "2014-06-03           47.729535             0.018565               0.020874   \n",
       "2014-06-04           40.041064             0.012218               0.019143   \n",
       "2014-06-05           40.751111             0.006907               0.016695   \n",
       "2014-06-06           36.478462            -0.000364               0.013284   \n",
       "2014-06-09           37.801890            -0.006654               0.009296   \n",
       "2014-06-10           39.189286            -0.010748               0.005287   \n",
       "\n",
       "            HG=F:Volume_ewm_short  HG=F:Volume_ewm_long  \n",
       "Date                                                     \n",
       "2014-05-28            1065.134806           1181.273277  \n",
       "2014-05-29            1039.513206           1164.444645  \n",
       "2014-05-30             992.899451           1136.659574  \n",
       "2014-06-02             949.422067           1109.851064  \n",
       "2014-06-03             903.975482           1081.495291  \n",
       "2014-06-04             865.718999           1056.233151  \n",
       "2014-06-05             826.382290           1029.995998  \n",
       "2014-06-06             800.293110           1010.061703  \n",
       "2014-06-09             765.370973            985.436729  \n",
       "2014-06-10             731.540588            961.028968  \n",
       "\n",
       "[10 rows x 85 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \"tickers_df\" not in dir():\n",
    "    tickers_df = pd.read_csv(\"./data/stock_ticker_data.csv\", header=[0,1])\n",
    "\n",
    "prediction_ticker = [\"Ticker\", \"AAPL\"]\n",
    "relevant_tickers = [\"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\"]\n",
    "global_market_tickers = [\"GC=F\", \"CL=F\", \"^GSPC\", \"^VIX\", \"DX-Y.NYB\", \"HG=F\"]\n",
    "\n",
    "training_df = parse_technical_indicators(tickers_df, prediction_ticker + relevant_tickers + global_market_tickers, \"AAPL\",\n",
    "                       datetime.datetime(2014, 1, 1), datetime.datetime(2023, 1, 1), short_period=30, long_period=60)\n",
    "\n",
    "# taking a look somewhere in the middle of this dataframe...\n",
    "training_df.iloc[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a view at what the technical indicator data looks like:\n",
    "plot_technical_indicators(training_df, datetime.datetime(2022, 1, 1), datetime.datetime(2023, 1, 1), \"aapl\")\n",
    "plot_technical_indicators(training_df, datetime.datetime(2022, 1, 1), datetime.datetime(2023, 1, 1), \"msft\")\n",
    "plot_technical_indicators(training_df, datetime.datetime(2022, 1, 1), datetime.datetime(2023, 1, 1), \"amzn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Congressional Trade Data for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"congressional_df\" not in dir():\n",
    "    congressional_df = pd.read_csv(\"./data/congressional_data.csv\")\n",
    "\n",
    "# understand how many relevant purchases and sales there are for each stock type\n",
    "# we will be creating some sort of metric to quantify these congressional trading activities\n",
    "plot_congressional_trades_bar(congressional_df, [\"AAPL\"])\n",
    "plot_congressional_trades_bar(congressional_df, [\"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_congressional_df = extract_congressional_trades_ticker(congressional_df, [\"AAPL\"] + relevant_tickers)\n",
    "\n",
    "training_congressional_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Tickers and Congressional Trading Data Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_aapl = (training_congressional_df[\"ticker\"] == \"AAPL\")\n",
    "\n",
    "aapl_congressional_df = training_congressional_df[is_aapl].copy()\n",
    "aapl_congressional_df.columns = [\"ticker\", \"purchase_aapl\", \"sale_aapl\"]\n",
    "aapl_congressional_df.drop(\"ticker\", axis=1, inplace=True)\n",
    "support_congressional_df = training_congressional_df[~is_aapl].copy()\n",
    "support_congressional_df.columns = [\"ticker\", \"purchase_support\", \"sale_support\"]\n",
    "support_congressional_df.drop(\"ticker\", axis=1, inplace=True)\n",
    "\n",
    "# there will be MANY NaN values in this dataframe because of the fact that there are a lot less\n",
    "# congressional trades than there are stock data points. Imputation / feature engineering will be\n",
    "# necessary down the line\n",
    "training_ticker_congressional_df = pd.merge(\n",
    "    training_df,\n",
    "    aapl_congressional_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "training_ticker_congressional_df = pd.merge(\n",
    "    training_ticker_congressional_df,\n",
    "    support_congressional_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# fill in all NaN values with zeros\n",
    "training_ticker_congressional_df[\"purchase_aapl\"].fillna(0, inplace=True)\n",
    "training_ticker_congressional_df[\"sale_aapl\"].fillna(0, inplace=True)\n",
    "training_ticker_congressional_df[\"purchase_support\"].fillna(0, inplace=True)\n",
    "training_ticker_congressional_df[\"sale_support\"].fillna(0, inplace=True)\n",
    "\n",
    "# finally, get rid of unecessary columns. we denoted useful columns with the _ keyword\n",
    "keep_cols = [c for c in training_ticker_congressional_df.columns if '_' in c] + [\"AAPL:Adj Close\"]\n",
    "\n",
    "\n",
    "training_ticker_congressional_df = training_ticker_congressional_df[keep_cols[1:] + [keep_cols[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_congressional_trades_timeseries(\n",
    "    training_ticker_congressional_df, start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2022, 1, 1), ticker=\"aapl\",\n",
    "    barwidth=5, title=\"AAPL Future Avg Price and Congressional Transaction Volume: AAPL\" \n",
    ")\n",
    "\n",
    "plot_congressional_trades_timeseries(\n",
    "    training_ticker_congressional_df, start=datetime.datetime(2020, 1, 1), end=datetime.datetime(2022, 1, 1), ticker=\"support\",\n",
    "    barwidth=5, title=\"AAPL Future Avg Price and Congressional Transaction Volume: Support Tickers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Further Data Cleaning</p>\n",
    "As of now, we have all the raw data and partially handled some data cleaning. However, there are a few redundant features in this dataframe and other features that we no longer need, like the actual stock price, or the actual number for volumes. we can take the technical indicator data to determine actual stock price movement, and capture the relationship of the current volume of trading to the EWM of volumes. Also, for congressional data, we will compute a heuristic that is like a trading \"signal\" that decays over time so that the purchase and sale signals can span over more days and create a higher breadth of congressional trading data covereage for all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for any sparse signal data, where you can smooth out the signal and maintain its relevance for\n",
    "# a certain number of days after the signal is given\n",
    "def exponential_decay_imputation(congressional_trade_volume: pd.Series, signal_decay: float = 0.9) -> pd.Series:\n",
    "    \n",
    "    # there is either a 10% decay or set the trade volume signal to the actual volume of trade\n",
    "    signal_transformed = []\n",
    "\n",
    "    for i in range(len(congressional_trade_volume)):\n",
    "        if i == 0:\n",
    "            signal_transformed.append(congressional_trade_volume[i])\n",
    "            continue\n",
    "        \n",
    "        new_signal = max(signal_decay * signal_transformed[-1], congressional_trade_volume[i])\n",
    "        signal_transformed.append(new_signal)\n",
    "\n",
    "    return pd.Series(signal_transformed, index=congressional_trade_volume.index)\n",
    "\n",
    "# capture the relationship between the MACD and signal line\n",
    "def macd_signal_crossover(\n",
    "        macd: pd.Series, signal: pd.Series, ticker: str, epsilon: int = 0.05, span: int = 30,\n",
    "        signal_decay: float = 0.9) -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    # figure out when a crossover happens\n",
    "    macd_diff = macd - signal\n",
    "\n",
    "    # case where the difference is right on the line, consider that a crossover\n",
    "    crossover = np.where(np.abs(macd_diff) <= epsilon, True, False)\n",
    "\n",
    "    # sometimes the crossover jumps from negative to positive or vice versa\n",
    "    macd_diff_shifted = macd_diff.shift(1, fill_value=0) # holds the previous value of the macd_diff on same location i \n",
    "    crossover = crossover | np.where(macd_diff * macd_diff_shifted < 0, True, False)\n",
    "\n",
    "    # figure out the EWM of the macd line and compare it to the EWM of the signal line\n",
    "    # if the macd line is above the signal line, that is a bullish crossover\n",
    "    # if the macd line is below the signal line, that is a bearish crossover\n",
    "    ewm_macd = macd.ewm(span=span, adjust=False).mean()\n",
    "    ewm_signal = signal.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    bullish_crossover = np.where(ewm_macd < ewm_signal, 1, 0)\n",
    "    bullish_crossover = pd.Series(bullish_crossover)  # Convert to Series before applying ewm\n",
    "    bullish_crossover = bullish_crossover & pd.Series(crossover)\n",
    "    # apply an exponential weighted mean to smooth out the data\n",
    "    # bullish_crossover = bullish_crossover.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    bearish_crossover = np.where(ewm_macd >= ewm_signal, 1, 0)\n",
    "    bearish_crossover = pd.Series(bearish_crossover)  # Convert to Series before applying ewm\n",
    "    bearish_crossover = bearish_crossover & pd.Series(crossover)\n",
    "    # apply an exponential weighted mean to smooth out the data\n",
    "    # bearish_crossover = bearish_crossover.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "\n",
    "    # finally, apply some sort of signal smoothing (since a crossover is most relevant on the day it happens, sure.\n",
    "    # however, it is also relevant in the days following the crossover up to a certain amount of time!)\n",
    "    # bullish_crossover = bullish_crossover.ewm(span=span, adjust=False).mean()\n",
    "    # bearish_crossover = bearish_crossover.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    bullish_crossover = exponential_decay_imputation(bullish_crossover, signal_decay=signal_decay)\n",
    "    bearish_crossover = exponential_decay_imputation(bearish_crossover, signal_decay=signal_decay)\n",
    "\n",
    "    bullish_crossover.index = macd.index\n",
    "    bearish_crossover.index = macd.index\n",
    "    macd_diff.index = macd.index\n",
    "\n",
    "\n",
    "    return bullish_crossover, bearish_crossover, macd_diff\n",
    "\n",
    "# handle volume columns to capture the relative difference between the current volume and the EWM of the volume\n",
    "def volume_ewm_proportion(volume: pd.Series, span: int = 30) -> pd.Series:\n",
    "    ewm_volume = volume.ewm(span=span, adjust=False).mean()\n",
    "    return volume / ewm_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = volume_ewm_proportion(training_ticker_congressional_df[\"AAPL:Volume_ewm_long\"], span=30)\n",
    "\n",
    "volume.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullish, bearish, macd_diff = macd_signal_crossover(\n",
    "    training_ticker_congressional_df[\"AAPL:Adj Close_macd\"], training_ticker_congressional_df[\"AAPL:Adj Close_signal\"], \"AAPL\",\n",
    "    epsilon=0.01, span=10, signal_decay=0.7)\n",
    "\n",
    "# plot the bullish and bearish crossover signals\n",
    "fig, ax = plt.subplots(figsize=(14, 7), nrows=4, ncols=1, sharex=True)\n",
    "\n",
    "start = datetime.datetime(2015, 1, 1)\n",
    "end = datetime.datetime(2016, 1, 1)\n",
    "iloc_series = (training_ticker_congressional_df.index >= start) & (training_ticker_congressional_df.index <= end)\n",
    "\n",
    "ax[0].plot(training_ticker_congressional_df[\"AAPL:Adj Close\"][iloc_series])\n",
    "ax[1].plot(macd_diff[iloc_series])\n",
    "ax[1].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "ax[2].plot(bullish[iloc_series], label=\"Bullish Crossover\", color=\"green\")\n",
    "ax[3].plot(bearish[iloc_series], label=\"Bearish Crossover\", color=\"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congressional_purchase = exponential_decay_imputation(training_ticker_congressional_df[\"purchase_aapl\"])\n",
    "congressional_sale = exponential_decay_imputation(training_ticker_congressional_df[\"sale_aapl\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7), nrows=3, ncols=1, sharex=True)\n",
    "\n",
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = datetime.datetime(2016, 1, 1)\n",
    "iloc_series = (training_ticker_congressional_df.index >= start) & (training_ticker_congressional_df.index <= end)\n",
    "\n",
    "ax[0].plot(training_ticker_congressional_df[\"AAPL:Adj Close\"][iloc_series])\n",
    "ax[1].plot(congressional_purchase[iloc_series], label=\"Congressional Purchase\", color=\"green\")\n",
    "ax[2].plot(congressional_sale[iloc_series], label=\"Congressional Sale\", color=\"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see how these signals looks like and are related to the movement of stock prices, we can just apply these to every column of the dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame(training_ticker_congressional_df[\"future_avg_price\"])\n",
    "all_colnames = training_ticker_congressional_df.columns\n",
    "all_relevant_tickers = set([colname.split(':')[0] for colname in all_colnames if ':' in colname])\n",
    "\n",
    "for ticker in all_relevant_tickers:\n",
    "    # for each ticker, get the macd signals, RSI, and volume signals\n",
    "\n",
    "    bullish, bearish, macd_diff = macd_signal_crossover(\n",
    "        training_ticker_congressional_df[f\"{ticker}:Adj Close_macd\"], training_ticker_congressional_df[f\"{ticker}:Adj Close_signal\"], \n",
    "        ticker, epsilon=0.01, span=10, signal_decay=0.7)\n",
    "    rsi = calculate_rsi(training_ticker_congressional_df[f\"{ticker}:Adj Close_rsi\"], period=14, use_exponential=True)\n",
    "    volume = volume_ewm_proportion(training_ticker_congressional_df[f\"{ticker}:Volume_ewm_long\"], span=30)\n",
    "    \n",
    "    ticker = ticker.lower()\n",
    "\n",
    "    training_df[f\"{ticker}:macd_diff\"] = macd_diff\n",
    "    training_df[f\"{ticker}:bullish\"] = bullish\n",
    "    training_df[f\"{ticker}:bearish\"] = bearish\n",
    "    training_df[f\"{ticker}:rsi\"] = rsi / 100 # scaling RSI here itself\n",
    "\n",
    "    if ticker == \"^vix\" or ticker == \"dx-y.nyb\":\n",
    "        continue\n",
    "\n",
    "    training_df[f\"{ticker}:volume_prop\"] = volume\n",
    "\n",
    "# next, add the congressional purchase/ sale signals already there\n",
    "training_df[\"purchase_aapl\"] = exponential_decay_imputation(training_ticker_congressional_df[\"purchase_aapl\"])\n",
    "training_df[\"sale_aapl\"] = exponential_decay_imputation(training_ticker_congressional_df[\"sale_aapl\"])\n",
    "training_df[\"purchase_support\"] = exponential_decay_imputation(training_ticker_congressional_df[\"purchase_support\"])\n",
    "training_df[\"sale_support\"] = exponential_decay_imputation(training_ticker_congressional_df[\"sale_support\"])\n",
    "\n",
    "# and our Y variable, the future average stock price for AAPL\n",
    "training_df[\"future_avg_price\"] = training_ticker_congressional_df[\"future_avg_price\"]\n",
    "training_df = training_df.dropna()\n",
    "\n",
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Understanding Correlations and Clustering Tendency of Our Stock Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a dataset we can work with. For this section we are doing data exploration again but this time it is exploring the clustering and relationships of one variable with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_matrix(training_df: pd.DataFrame) -> None:\n",
    "    X = training_df\n",
    "\n",
    "    correlation_matrix = X.corr()\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Create a heatmap to visualize the correlation matrix\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5,\n",
    "                xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns)\n",
    "\n",
    "    # Set the title of the heatmap\n",
    "    plt.title('Correlation Matrix')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation_matrix(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this correlation matrix, we definitely have a high relationship of variables for metrics such as the MACD and RSI variables. However, for data such as congressional trading signals and the MACD bullish/ bearish signals, they are not significantly correlated with other continuous variables. This is fine for our purposes because that is expected of the signal data due to their sparse/ rare nature. One thing to note is that models such as decision trees or neural nets are pretty versatile and can handle sparse data well, so for the purposes of this project we will keep those values in.\n",
    "\n",
    "we can still use this matrix to prune out other indicator data that do not have significant relationships with anything else in the dataset. this can be done by visually selecting out the variables that we dont need anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually looking at this matrix, we can remove out cl=f, hg=f, & gc=f since they dont seem to correlate well with anything anyways\n",
    "\n",
    "remove = set([\n",
    "    \"cl=f\", \"hg=f\", \"gc=f\", \"dx-y.nyb\"\n",
    "])\n",
    "\n",
    "def contains_remove(colname: str) -> bool:\n",
    "    for rem in remove:\n",
    "        if rem in colname:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "cols_to_keep = [colname for colname in training_df.columns if not contains_remove(colname)]\n",
    "\n",
    "# a slightly cleaner correlation matrix, and these are all the symbols that we will be using for our model\n",
    "plot_correlation_matrix(training_df[cols_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so given that we now know the relevant columns, we can finalize our training dataframe to have these relevant features\n",
    "training_df = training_df[cols_to_keep]\n",
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more piece of data exploration that is worth looking into is the clustering tendency of all the data we already have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pca_dataset(training_df: pd.DataFrame, threshold: float = 0.2):\n",
    "    # Separate features (X) and target variable (y)\n",
    "    X = training_df.drop(\"future_avg_price\", axis=1)\n",
    "    y_below = training_df[\"future_avg_price\"] <= -threshold\n",
    "    y_above = training_df[\"future_avg_price\"] > threshold\n",
    "    y_middle = ~y_below & ~y_above\n",
    "    y = training_df[\"future_avg_price\"] > threshold\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Explained variance for each component\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    # Total explained variance\n",
    "    total_variance = explained_variance.sum()\n",
    "\n",
    "    # Colors based on the target (future average price threshold)\n",
    "    colors = ['red' if label else 'blue' for label in y]\n",
    "\n",
    "    # Plot the PCA components\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors, alpha=0.6)\n",
    "\n",
    "    # Set the title with explained variance\n",
    "    plt.title(f\"PCA of Training Data\\n\"\n",
    "            f\"Total Explained Variance: {total_variance:.2f} \"\n",
    "            f\"(PC1: {explained_variance[0]:.2f}, PC2: {explained_variance[1]:.2f})\")\n",
    "\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_dataset(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't an obvious clustering between good or bad stocks in this analysis. This is expected because after all, we are dealing with the stock market here. It is difficult to predict the stock market, and this PCA definitely reflects that issue well. Given this PCA plot, we don't expect our model to be perfect, but we still hope to see the model being able to predict the general trend/ direction of stock price movements given all the data provided to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "We will try two models: Random Forests and Neural Networks.\n",
    "\n",
    "The first part of this notebook explores regression via a Random Forest Regressor and a regressor Neural Network. The results from this revealed to us that trying to employ regression on stock market data is very difficult.\n",
    "\n",
    "Then, we pivot our approach to classification...\n",
    "\n",
    " For actual model training, because we are using time series data, we have to modify our cross validation pipeline to not just pick random samples, but basically train on data that it has seen before and then predict future prices. this is important for the prevention of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y, both sharing the same timeseries index\n",
    "X = training_df.drop(\"future_avg_price\", axis=1)\n",
    "y = training_df[\"future_avg_price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Random Forest Regressor\n",
    "\n",
    "important points here is that you need to do a time series for cross validation because you need to ensure that past data is used to predict future. otherwise, there will be sources of data leakage which is not good!\n",
    "\n",
    "\n",
    "We can try using either regression or classification and see what works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"rf\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [20, 50, 100, 200],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4, 8, 16],\n",
    "    'rf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_time_split_inner = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rf_inner_grid_search = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=rf_time_split_inner,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "rf_time_split_outer = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores, r2_scores, explained_var_scores = [], [], []\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 15), nrows=5, ncols=1)\n",
    "i = 0\n",
    "\n",
    "for train_index, test_index in rf_time_split_outer.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    rf_inner_grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_pipeline = rf_inner_grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "\n",
    "    # accuracy\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    # precision\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    # recall\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    explained_var_scores.append(explained_var)\n",
    "\n",
    "    ax[i].set_title(f\"Time Series Split {i + 1}\")\n",
    "    ax[i].plot(y_test, label=\"Actual\")\n",
    "    ax[i].plot(y_pred, label=\"Predicted\")\n",
    "    ax[i].set_xlabel(\"Date\")\n",
    "    ax[i].set_ylabel(\"Price Change %\")\n",
    "    ax[i].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so how did we do?\n",
    "print(\"Summary Statistics of Random Forest Regression:\")\n",
    "print(f\"Mean Squared Error: {np.mean(mse_scores)}\")\n",
    "print(f\"R2 Score: {np.mean(r2_scores)}\")\n",
    "print(f\"Explained Variance Score: {np.mean(explained_var_scores)}\")\n",
    "\n",
    "print(f\"Best Parameters: {rf_inner_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at these results, it is pretty clear that the model does not do the greatest job at perfectly predicting the stock price. This is expected. It is incredibly hard to put a number on the future movement of a stock price, especially if crucial features of technical indicators are not present like congressional trading purchase signals or MACD line crossing.\n",
    "\n",
    "Despite not performing well for regression (~0.17 R2 score), it is still notable how the overall direction / trend of stock price movements seem to be reflected in our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Neural Network Regressor\n",
    "\n",
    "A Neural Network is comparable to a random forest classifier because it excels at capturing complex, non-linear patterns. In addition, these sorts of models also handle larger amounts of features better than if you were to use a random forest classifier, which might suffer from the curse of dimensionality for features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a very similar way as the random forest regressor, we can create a neural network\n",
    "# regressor on time series cross validation data.\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "nn_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),  # Scaling the features\n",
    "    (\"nn\", MLPRegressor(random_state=42))  # Neural Network Regressor\n",
    "])\n",
    "\n",
    "# Hyperparameter grid for MLPRegressor\n",
    "nn_param_grid = {\n",
    "    'nn__hidden_layer_sizes': [\n",
    "        (20,),\n",
    "        (50,),\n",
    "        (70,),\n",
    "        (20, 10), \n",
    "        (50, 10), \n",
    "        (70, 10),\n",
    "        (20, 20),\n",
    "        (50, 20),\n",
    "        (70, 20)],\n",
    "    'nn__activation': ['relu'],\n",
    "    'nn__solver': ['adam', 'sgd'],\n",
    "    'nn__learning_rate': ['constant', \"invscaling\"],\n",
    "    'nn__max_iter': [200, 400, 800]\n",
    "}\n",
    "\n",
    "# TimeSeries cross-validation for inner loop grid search\n",
    "nn_time_split_inner = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# GridSearchCV for tuning the MLP regressor hyperparameters\n",
    "nn_inner_grid_search = GridSearchCV(\n",
    "    estimator=nn_pipeline,\n",
    "    param_grid=nn_param_grid,\n",
    "    cv=nn_time_split_inner,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# TimeSeries cross-validation for outer loop to evaluate performance\n",
    "nn_time_split_outer = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mse_scores, r2_scores, explained_var_scores = [], [], []\n",
    "\n",
    "# Create subplots for visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 15), nrows=5, ncols=1)\n",
    "i = 0\n",
    "\n",
    "# Outer loop: time series cross-validation for model evaluation\n",
    "for train_index, test_index in nn_time_split_outer.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the grid search on the training data\n",
    "    nn_inner_grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best pipeline from grid search\n",
    "    best_pipeline = nn_inner_grid_search.best_estimator_\n",
    "\n",
    "    # Predict using the best model\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "\n",
    "    # Calculate and store evaluation metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "    explained_var = explained_variance_score(y_test, y_pred)\n",
    "    explained_var_scores.append(explained_var)\n",
    "\n",
    "    # Plot actual vs predicted for each fold\n",
    "    ax[i].set_title(f\"Time Series Split {i + 1}\")\n",
    "    ax[i].plot(y_test, label=\"Actual\")\n",
    "    ax[i].plot(y_pred, label=\"Predicted\")\n",
    "    ax[i].set_xlabel(\"Date\")\n",
    "    ax[i].set_ylabel(\"Price Change %\")\n",
    "    ax[i].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics of Neural Network Regression:\")\n",
    "print(f\"Mean Squared Error: {np.mean(mse_scores)}\")\n",
    "print(f\"R2 Score: {np.mean(r2_scores)}\")\n",
    "print(f\"Explained Variance Score: {np.mean(explained_var_scores)}\")\n",
    "\n",
    "print(f\"Best Parameters: {nn_inner_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using neural networks seemed to bring us significantly worse results. (explain why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, using regression for stock price prediction is challenging. The very low R2 values reflect that even if the stock price prediction is in the right direction, the actual distance between actual and predicted stock price values is still too different. \n",
    "\n",
    "However, we realized that even if regression for Random Forests and Neural networks is not able to predict actual price values of stocks, they do tend to get the direction of price correct some of the time.\n",
    "\n",
    "We can try to take advantage of this propery of stock price predictions and attempt to build a classification model. So let us try that next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, transform the numerical Y values into something that a classifier can handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "thresh = 0.8\n",
    "\n",
    "y_categorical_buy = y > thresh\n",
    "y_categorical_neutral = (y <= thresh) & (y >= -thresh)\n",
    "y_categorical_sell = y < -thresh\n",
    "\n",
    "# y can represent multiple classes, but we can use an integer to\n",
    "# represent each class. For example, 0 for sell, 1 for neutral, and 2 for buy\n",
    "\n",
    "# this is because scikit learn classifiers require integer labels\n",
    "\"\"\" \n",
    "    sell = 0\n",
    "    neutral = 1\n",
    "    buy = 2\n",
    "\"\"\"\n",
    "y_categorical = y_categorical_sell.astype(int) + y_categorical_neutral.astype(int) * 2 + y_categorical_buy.astype(int) * 3\n",
    "\n",
    "# take a look at what these value distributions look like\n",
    "y_categorical.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are a lot more sell signals than there are buy or neutral signals. how do we handle this data imbalance?\n",
    "\n",
    "there is a class_weight parameter in the RandomForestClassifier object, and this will assign a varying penalization factor based on the frequencies of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"rf\", RandomForestClassifier(random_state=42, class_weight=\"balanced\", n_jobs=-1))\n",
    "])\n",
    "\n",
    "# share the same rf_param_grid as before, but I am explicitly setting it\n",
    "# here as well for clarity\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [100],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [4],\n",
    "    'rf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Define TimeSeriesSplit for inner cross-validation\n",
    "rf_time_split_inner = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# GridSearchCV for parameter tuning with time series split\n",
    "rf_inner_grid_search = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=rf_time_split_inner,\n",
    "    scoring=\"accuracy\",  # Use classification score (e.g., accuracy)\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Define TimeSeriesSplit for outer cross-validation\n",
    "rf_time_split_outer = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Store metrics for evaluation\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "y_actuals = []\n",
    "y_preds = []\n",
    "\n",
    "# Outer loop for time series cross-validation\n",
    "for train_index, test_index in rf_time_split_outer.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    # y changed to y_categorical here!!!\n",
    "    y_train, y_test = y_categorical.iloc[train_index], y_categorical.iloc[test_index]\n",
    "\n",
    "    # Fit the inner grid search to find the best parameters\n",
    "    rf_inner_grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best pipeline from the grid search\n",
    "    best_pipeline = rf_inner_grid_search.best_estimator_\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    y_pred = pd.Series(y_pred, index=y_test.index)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    precision_scores.append(precision)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    y_actuals.append(y_test)\n",
    "    y_preds.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_actuals)):\n",
    "    print(confusion_matrix(y_actuals[i], y_preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also talk about precision/ recall scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some very impressive results for accuracy scores for stock predictions. However, in this case, we need to know how many of each category our model is getting right. It is easy to just predict \"sell\" for everything and be right most of the time, but we want to make sure that the \"buy\" is specifically high in accuracy as well as the \"sell\". If you are in \"neutral\", then assume that the stock price prediction is not able to confidently be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(model_name, y_true, y_pred):\n",
    "    global evaluation_results\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Append metrics to the DataFrame\n",
    "    evaluation_results = pd.concat(\n",
    "        [\n",
    "            evaluation_results,\n",
    "            pd.DataFrame({\n",
    "                \"Model\": [model_name],\n",
    "                \"Accuracy\": [accuracy],\n",
    "                \"Precision\": [precision],\n",
    "                \"Recall\": [recall],\n",
    "                \"F1 Score\": [f1]\n",
    "            })\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasVarianceAnalyzer(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.base_model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return accuracy_score(y, predictions)\n",
    "\n",
    "    def evaluate_bias_variance(self, X_train, y_train, X_test, y_test):\n",
    "        train_predictions = self.base_model.predict(X_train)\n",
    "        test_predictions = self.base_model.predict(X_test)\n",
    "\n",
    "        train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "        test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "        # Bias-variance analysis\n",
    "        if train_accuracy < 0.8:\n",
    "            print(\"High bias detected: Consider increasing complexity.\")\n",
    "        elif train_accuracy > 0.95 and (train_accuracy - test_accuracy) > 0.1:\n",
    "            print(\"High variance detected: Consider regularization or simplifying model.\")\n",
    "        else:\n",
    "            print(\"Model shows a reasonable bias-variance tradeoff.\")\n",
    "\n",
    "        return train_accuracy, test_accuracy\n",
    "\n",
    "# Pipeline definition\n",
    "def build_pipeline():\n",
    "    knn = KNeighborsClassifier()\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    final_estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    base_learners = [\n",
    "        ('knn', knn),\n",
    "        ('svm', svm)\n",
    "    ]\n",
    "    stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=final_estimator)\n",
    "\n",
    "    # Parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'knn__n_neighbors': range(1, 11),\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__metric': ['euclidean', 'manhattan'],\n",
    "        'svm__C': [0.1, 1, 10, 100],\n",
    "        'svm__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'svm__gamma': ['scale', 'auto'],\n",
    "        'svm__degree': [3, 4, 5],\n",
    "        'final_estimator__C': [0.1, 1, 10, 100],\n",
    "        'final_estimator__solver': ['lbfgs', 'liblinear'],\n",
    "    }\n",
    "\n",
    "    # Cross-validation strategy\n",
    "    inner_cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=stacking_clf,\n",
    "        param_grid=param_grid,\n",
    "        cv=inner_cv,\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_stacking_clf = grid_search.best_estimator_\n",
    "\n",
    "    analyzer = BiasVarianceAnalyzer(best_stacking_clf)\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('model_training', analyzer)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Example usage\n",
    "pipeline = build_pipeline(X_train, y_train)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate bias-variance\n",
    "analyzer = pipeline.named_steps['model_training']\n",
    "train_accuracy, test_accuracy = analyzer.evaluate_bias_variance(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do the same process as above, but now with adaboosting, then with a neural network classifier. and then try on an SVM\n",
    "\n",
    "talk about accuracy scores, precision, recall, and confusion matrices for each thing and explain what that means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after everything, have some table comparing all models regressors and classifications\n",
    "done with modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockpredictions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
